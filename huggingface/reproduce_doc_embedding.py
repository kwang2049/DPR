import torch
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

# The very first passage in the collection `data.wikipedia_split.psgs_w100`
# The results from running the original DPR code: (first 10 dimensions)
#   array([ 0.00213145,  0.54385525, -0.3342023 , -0.6717472 , -0.55628437,
#     0.52181476,  0.7358721 , -0.10325794, -0.26405236, -0.4588768 ], dtype=float32)
# Note that this is slightly different from the pre-encoded embedding files, i.e. `data.retriever_results.nq.single.wikipedia_passages
# For more details, please refer to https://github.com/facebookresearch/DPR/issues/208
# **** Note that, it is found that both devices (CPU/GPU) and batch_size can influence the final results!!! ****

title = "Aaron"
passage = """Aaron Aaron ( or ; "Ahärôn") is a prophet, high priest, and the brother of Moses in the Abrahamic religions. Knowledge of Aaron, along with his brother Moses, comes exclusively from religious texts, such as the Bible and Quran. The Hebrew Bible relates that, unlike Moses, who grew up in the Egyptian royal court, Aaron and his elder sister Miriam remained with their kinsmen in the eastern border-land of Egypt (Goshen). When Moses first confronted the Egyptian king about the Israelites, Aaron served as his brother's spokesman ("prophet") to the Pharaoh. Part of the Law (Torah) that Moses received from"""

tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')

inputs = tokenizer(
    [title],
    text_pair=[passage],
    add_special_tokens=True,
    max_length=256,
    pad_to_max_length=True,
    truncation=True,
    return_tensors='pt'
)

input_ids = inputs['input_ids'][0].tolist()
input_ids[-1] = tokenizer.sep_token_id  # So this is the key to give it a full reproduction. It seems to be a bug in the official code: https://github.com/facebookresearch/DPR/issues/210
input_ids = torch.LongTensor([input_ids])

token_type_ids = torch.zeros_like(input_ids)
attn_mask = (input_ids != tokenizer.pad_token_id).long()

print(inputs)
print(input_ids)
print(token_type_ids)  # Note here this token_type_ids used in DPR's official code is very different from that generated by Huggingface's tokenizer directly!!!
print(attn_mask)
# tensor([[  101,  7158,   102,  7158,  7158,  1006,  2030,  1025,  1000,  6289,
#          10464,  2078,  1000,  1007,  2003,  1037, 12168,  1010,  2152,  5011,
#           1010,  1998,  1996,  2567,  1997,  9952,  1999,  1996,  8181,  2594,
#          11822,  1012,  3716,  1997,  7158,  1010,  2247,  2007,  2010,  2567,
#           9952,  1010,  3310,  7580,  2013,  3412,  6981,  1010,  2107,  2004,
#           1996,  6331,  1998, 21288,  1012,  1996,  6836,  6331, 14623,  2008,
#           1010,  4406,  9952,  1010,  2040,  3473,  2039,  1999,  1996,  6811,
#           2548,  2457,  1010,  7158,  1998,  2010,  6422,  2905, 16925,  2815,
#           2007,  2037, 12631,  6491,  2368,  1999,  1996,  2789,  3675,  1011,
#           2455,  1997,  5279,  1006,  2175,  4095,  2368,  1007,  1012,  2043,
#           9952,  2034, 12892,  1996,  6811,  2332,  2055,  1996,  5611,  4570,
#           1010,  7158,  2366,  2004,  2010,  2567,  1005,  1055, 14056,  1006,
#           1000, 12168,  1000,  1007,  2000,  1996, 22089,  1012,  2112,  1997,
#           1996,  2375,  1006, 16297,  1007,  2008,  9952,  2363,  2013,   102,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,   102]])
# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])


model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
model.eval()

with torch.no_grad():
    out = model(input_ids, attn_mask, token_type_ids)

print(out.pooler_output[0][:10].numpy())
# [ 0.00213145  0.54385525 -0.3342023  -0.6717472  -0.55628437  0.52181476
#   0.7358721  -0.10325794 -0.26405236 -0.4588768 ]